---
title: "Problem Set 4"
author: "Partner1: Sienna Wang, Partner2: Hengyi Xing"
date: "October 28, 2024"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
execute:
  eval: true
  echo: true
  warning: false
---

1. This submission is our work alone and complies with the 30538 integrity policy. HX & SW  
2. I have uploaded the names of anyone I worked with on the problem set **[here](https://docs.google.com/forms/d/1-zzHx762odGlpVWtgdIC55vqF-j3gqdAp6Pno1rIGK0/edit)**. HX & SW  
3. Late coins used this pset: 1. Late coins left after submission: 2 

## Style Points (10 pts)

## Submission Steps (10 pts)
```{python}
# Set up
import pandas as pd
import altair as alt
alt.renderers.enable("png")
import geopandas as gpd
import matplotlib.pyplot as plt
import time 
from matplotlib.colors import ListedColormap
```

# Download and explore the Provider of Services (POS) file (10 pts)

## 1.  
We will use `PRVDR_CTGRY_SBTYP_CD` and `PRVDR_CTGRY_CD` to focus on short-term hospitals, `PRVDR_NUM` (CMS certification number) to identify unique hospitals, `PGM_TRMNTN_CD` to identify hospitals that are suspected to have closed, `FAC_NAME` to get the facility name, and `ZIP_CD` to get the ZIP.  
  
## 2. (a)  
```{python}
# To import pos2016
path = ('/Users/wangshiying/Documents/71_Python_Programming_II/'
        'problem-set-4-hengyi-and-sienna')
file = "/data/pos2016.csv"

df_pos2016 = pd.read_csv(path + file)

# To focus on short-term hospitals
df_2016 = df_pos2016[(df_pos2016["PRVDR_CTGRY_SBTYP_CD"] == 1)
                     & (df_pos2016["PRVDR_CTGRY_CD"] == 1)]

# To count the observations
len(df_2016)
```

Therefore, 7,245 hospitals are reported in the data. This number seems a bit larger than expected.

## 2. (b)  
From [Definite Healthcare](https://www.definitivehc.com/blog/how-many-hospitals-are-in-the-us), there are only 3,873 short-term hospitals in the US in 2024. And by American Hospital Association, the total number of all US hospitals is 6,120 in [2024](https://www.aha.org/statistics/fast-facts-us-hospitals) and only 5,534 in [2016](https://www.aha.org/statistics/2018-01-09-fast-facts-us-hospitals-2018-pie-charts).  
We can find from the above information that the number of short-term hospitals we get from our dataset is even larger than the total number of hospitals in the US. As for reasons, firstly, the dataset might include multiple entries for the same hospital, such as separate records for different departments or units within the same facility (there are only 6770 different facility names). Secondly, CMS may categorize certain facilities as "short-term hospitals" even if they wouldn’t be considered standalone hospitals in AHA or other national data.

## 3. 
```{python}
# To import datasets
file = "/data/pos2017.csv"
df_pos2017 = pd.read_csv(path + file)
file = "/data/pos2018.csv"
df_pos2018 = pd.read_csv(path + file, encoding="ISO-8859-1")
file = "/data/pos2019.csv"
df_pos2019 = pd.read_csv(path + file, encoding="ISO-8859-1")

# To focus on short-term hospitals
df_2017 = df_pos2017[(df_pos2017["PRVDR_CTGRY_SBTYP_CD"] == 1)
                     & (df_pos2017["PRVDR_CTGRY_CD"] == 1)]
df_2018 = df_pos2018[(df_pos2018["PRVDR_CTGRY_SBTYP_CD"] == 1)
                     & (df_pos2018["PRVDR_CTGRY_CD"] == 1)]
df_2019 = df_pos2019[(df_pos2019["PRVDR_CTGRY_SBTYP_CD"] == 1)
                     & (df_pos2019["PRVDR_CTGRY_CD"] == 1)]

# To append them together
df_2016["YEAR"] = 2016
df_2017["YEAR"] = 2017
df_2018["YEAR"] = 2018
df_2019["YEAR"] = 2019
df = pd.concat([df_2016, df_2017, df_2018, df_2019], axis=0, ignore_index=True)

# Plot the number of observations by year
alt.data_transformers.enable("vegafusion")
alt.Chart(df).mark_bar(size=40).encode(
    alt.X("YEAR:O", title="Year", axis=alt.Axis(labelAngle=0)),
    alt.Y("count()", title="Number of Observations", scale=alt.Scale(domain=(7000, 7400))) # Set domain to make the trend more obvious
).properties(
  width = 500,
  height = 180
)
```

## 4. (a)
```{python}
# Find the number of unique hospitals
unique_number_by_year = df.groupby("YEAR")["PRVDR_NUM"].nunique().reset_index()

# Plot the number of unique hospitals
alt.Chart(df).mark_bar(size=40).encode(
    alt.X("YEAR:O", title="Year", axis=alt.Axis(labelAngle=0)),
    alt.Y("distinct(PRVDR_NUM):Q", title="Number of Unique Observations", scale=alt.Scale(domain=(7000, 7400))) # Set domain to make the trend more obvious
).properties(
  width = 500,
  height = 180
)
```
## 4. (b)
Comparing this with the previous step, we can find that there is no obvious difference. We can learn that in this dataset, each record represents one distinct hospital. Also, it implies that each CMS certification number appears multiple times over years. Therefore, this is an unbalanced panel dataset.

# Identify hospital closures in POS file (15 pts) (*)

## 1. 
```{python}
# Find active facilities in each year
active_2016 = (df_2016[df_2016['PGM_TRMNTN_CD'] == 0]
              [['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']].reset_index(drop=True))
active_2017 = (df_2017[df_2017['PGM_TRMNTN_CD'] == 0]
              [['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']].reset_index(drop=True))
active_2018 = (df_2018[df_2018['PGM_TRMNTN_CD'] == 0]
              [['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']].reset_index(drop=True))
active_2019 = (df_2019[df_2019['PGM_TRMNTN_CD'] == 0]
              [['PRVDR_NUM', 'FAC_NAME', 'ZIP_CD']].reset_index(drop=True))


# Define a function to identify suspected closure in each year
def find_closed_facilities(active_year_before, active_year_after):
    '''
    Identify records in one year yet missing in the next year, using merge. 
    Input arguments are two dataframes above and the suspected closure year.
    '''
    result = pd.merge(active_year_before, active_year_after, 
                      on=['PRVDR_NUM'], how='outer', indicator=True)
    result = result[result['_merge'] == 'left_only']
    return result


# Apply to each year from 2017 to 2019
closed_2017 = find_closed_facilities(active_2016, active_2017)
closed_2018 = find_closed_facilities(active_2016, active_2018)
closed_2019 = find_closed_facilities(active_2016, active_2019)

# Rectify 2018 and 2019 data to include only the exact year
closed_2018 = pd.concat([closed_2017, closed_2018]).drop_duplicates(keep=False)
closed_2019 = pd.concat([closed_2017, closed_2019]).drop_duplicates(keep=False)
closed_2019 = pd.concat([closed_2018, closed_2019]).drop_duplicates(keep=False)
closed_2017['SUS_YEAR'] = 2017
closed_2018['SUS_YEAR'] = 2018
closed_2019['SUS_YEAR'] = 2019

# Connect all three years together
closed_2016_to_2019 = pd.concat([closed_2017, closed_2018, closed_2019])
closed_count = len(closed_2016_to_2019)
print(closed_count, 
      'facilities active in 2016 were suspected to have closed by 2019.')   
```

## 2. 
```{python}
# Modify the column name to FAC_NAME
closed_2016_to_2019['FAC_NAME'] = closed_2016_to_2019['FAC_NAME_x']
print(closed_2016_to_2019[['FAC_NAME', 'SUS_YEAR']]
      .sort_values('FAC_NAME', ignore_index=True).head(10))

```

## 3. (a)
Firstly, check whether the closure is a potential merger/acquisition by a simple "non-decrease" approach.
```{python}
# Check the suspected closure in 2017
zip_closed_2017 = closed_2017['ZIP_CD_x'].tolist()
ma_zip_2017 = []
for zip_code in zip_closed_2017:
  zip_count_2017 = len(active_2017[active_2017['ZIP_CD'] == zip_code])
  zip_count_2018 = len(active_2018[active_2018['ZIP_CD'] == zip_code])
  if zip_count_2017 <= zip_count_2018:
    ma_zip_2017.append(zip_code)
ma_2017 = closed_2017[closed_2017['ZIP_CD_x'].isin(ma_zip_2017)]

# Check the suspected closure in 2018
zip_closed_2018 = closed_2018['ZIP_CD_x'].tolist()
ma_zip_2018 = []
for zip_code in zip_closed_2018:
  zip_count_2018 = len(active_2018[active_2018['ZIP_CD'] == zip_code])
  zip_count_2019 = len(active_2019[active_2019['ZIP_CD'] == zip_code])
  if zip_count_2018 <= zip_count_2019:
    ma_zip_2018.append(zip_code)
ma_2018 = closed_2018[closed_2018['ZIP_CD_x'].isin(ma_zip_2018)]

ma_2017_2018_a = pd.concat([ma_2017, ma_2018])
ma_count_a = len(ma_2017_2018_a)
print(f'By a simple "non-decrease" approach, {ma_count_a} facilities fit the definition.')  
```
Secondly, by further consideration, it is reasonable to remove records where the number of hospitals is still 0 in the year after. These ZIP codes experienced "non-decrease", but there is no hospitals reappear in the year after.
```{python}
# Check the suspected closure in 2017
zip_closed_2017 = closed_2017['ZIP_CD_x'].tolist()
ma_zip_2017 = []
for zip_code in zip_closed_2017:
  zip_count_2017 = len(active_2017[active_2017['ZIP_CD'] == zip_code])
  zip_count_2018 = len(active_2018[active_2018['ZIP_CD'] == zip_code])
  if (zip_count_2017 <= zip_count_2018) & (zip_count_2018 != 0):
    ma_zip_2017.append(zip_code)
ma_2017 = closed_2017[closed_2017['ZIP_CD_x'].isin(ma_zip_2017)]

# Check the suspected closure in 2018
zip_closed_2018 = closed_2018['ZIP_CD_x'].tolist()
ma_zip_2018 = []
for zip_code in zip_closed_2018:
  zip_count_2018 = len(active_2018[active_2018['ZIP_CD'] == zip_code])
  zip_count_2019 = len(active_2019[active_2019['ZIP_CD'] == zip_code])
  if (zip_count_2018 <= zip_count_2019) & (zip_count_2019 != 0):
    ma_zip_2018.append(zip_code)
ma_2018 = closed_2018[closed_2018['ZIP_CD_x'].isin(ma_zip_2018)]

ma_2017_2018_b = pd.concat([ma_2017, ma_2018])
ma_count_b = len(ma_2017_2018_b)
print(f'Removing ZIP codes with no hospitals reappearing, {ma_count_b} facilities fit the definition.')  
```

## 3. (b)
```{python}
left_count = closed_count - ma_count_a
print(f'By a simple "non-decrease" approach, {left_count} hospitals are left.')
left_count = closed_count - ma_count_b
print(f'Take ZIP codes with no hospitals reappearing into consideration, {left_count} hospitals are left.')
```

## 3. (c)
Without considering ZIP codes with no hospitals reappearing, the table is shown below:
```{python}
# Remove ma_2017_2018 from closed_2016_to_2019
combined = pd.concat([closed_2016_to_2019, ma_2017_2018_a])
corrected_closed = combined.drop_duplicates('PRVDR_NUM', keep=False)
# Modify the column name to ZIP_CD
corrected_closed['ZIP_CD'] = corrected_closed['ZIP_CD_x']
print(corrected_closed[['FAC_NAME', 'SUS_YEAR']]
      .sort_values('FAC_NAME', ignore_index=True).head(10))
```
Take ZIP codes with no hospitals reappearing into consideration, the table is shown below:
```{python}
# Remove ma_2017_2018 from closed_2016_to_2019
combined = pd.concat([closed_2016_to_2019, ma_2017_2018_b])
corrected_closed = combined.drop_duplicates('PRVDR_NUM',keep=False)
# Modify the column name to ZIP_CD
corrected_closed['ZIP_CD'] = corrected_closed['ZIP_CD_x']
print(corrected_closed[['FAC_NAME', 'SUS_YEAR']]
      .sort_values('FAC_NAME', ignore_index=True).head(10))
```

# Download Census zip code shapefile (10 pt) 

## 1. (a)
-  **`.shp`** is the shape file storing geometric data, such as points, lines, or polygons.  
-  **`.shx`** is an index file for quick access to `.shp` records.  
-  **`.dbf`** holds attribute data related to each geographic feature.  
-  **`.prj`** defines the coordinate system and map projection for accurate spatial representation.
-  **`.xml`** contains metadata describing the dataset's content, source, and creation details.  

## 1. (b)
After unzipping, `.shp` file is 837.5 MB, `.shx` file is 265 KB, `.dbf` file is 6.4 MB, `.prj` file is 165 bytes, and `.xml` file is 16 KB.

## 2. 
```{python}
file = "/data/gz_2010_us_860_00_500k/gz_2010_us_860_00_500k.shp"
gdf_whole = gpd.read_file(path + file)
```
It can be found that for TX, the first 3 digits of ZIP code should be 750-799.
```{python}
# Filter the geodataframe for TX
gdf = gdf_whole[gdf_whole["ZCTA5"].str.startswith(
    ("75", "76", "77", "78", "79"))]

# Count the number of hospitals by ZIP code
hospitals_number = df_2016.groupby("ZIP_CD")["PRVDR_NUM"].count().reset_index()
hospitals_number.columns = ["ZCTA5", "COUNTS"]

# Merge into the geodataframe
gdf["ZCTA5"] = pd.to_numeric(gdf["ZCTA5"])
gdf = gdf.merge(hospitals_number, on="ZCTA5", how="left").fillna(0)

# Make the plot
fig, ax = plt.subplots(figsize=(12, 12))

ax = gdf.plot(column="COUNTS",
         cmap="Blues",
         legend=True,
         legend_kwds={"label": "Number of Hospitals"},
         linewidth=0.1,
         edgecolor="grey",
         ax=ax
         )
ax.set_title("Texas ZIP codes by Number of Active Hospitals")
cbar = ax.get_figure().get_axes()[-1]
cbar.spines[:].set_visible(False)
plt.axis("off")
plt.show()
```

# Calculate zip code’s distance to the nearest hospital (20 pts) (*)

## 1. 
```{python}
gdf_whole['centroid'] = gdf_whole.centroid
zips_all_centroids = gdf_whole[['ZCTA5', 'centroid']]
print(zips_all_centroids.shape)
print('We have 33120 rows and 2 columns. ZCTA5 is the postal zip codes of districts in the U.S.')
print('ZCTA5 is the approximate postal zip codes of districts in the U.S..')
print('''Centroid represents the position of center point of the area covered by a specific zip code in the Geographic Coordinate System: GCS_North_American_1983, with Datum being D_North_American_1983 and GRS_1980 spheroid and Greenwich prime meridian.''')
```

## 2. 
```{python}
zips_texas_centroids = (zips_all_centroids[zips_all_centroids['ZCTA5']
                        .str.startswith(("75", "76", "77", "78", "79"))])
zip_texas = zips_texas_centroids['ZCTA5'].nunique()
print(f'{zip_texas} unique zip codes in zips_texas_centroids.')
# States boadering Texas: 
# New Mexico: 870-884
# Oklahoma: 73-74
# Arkansas: 716-729
# Louisiana: 700-715
# Texas: 75 - 79
# Combined: 70-79, 870-884
prefixes_70_to_79 = [f"{i}" for i in range(70, 80)]
prefixes_870_to_884 = [f"{i}" for i in range(870, 885)]
prefixes_tuple = tuple(prefixes_70_to_79 + prefixes_870_to_884)
zips_texas_borderstates_centroids = (zips_all_centroids[zips_all_centroids['ZCTA5']
                        .str.startswith(prefixes_tuple)])
zip_boarders = zips_texas_borderstates_centroids['ZCTA5'].nunique()
print(f'{zip_boarders} unique zip codes in zips_texas_borderstates_centroids.')
```

## 3. 
```{python}
zips_texas_borderstates_centroids["ZCTA5"] = pd.to_numeric(zips_texas_borderstates_centroids["ZCTA5"])
zips_withhospital_centroids = pd.merge(zips_texas_borderstates_centroids, active_2016, 
                                      left_on='ZCTA5', right_on='ZIP_CD', how='inner', indicator=True
                                      ).drop_duplicates('ZCTA5')
zips_withhospital_centroids = zips_withhospital_centroids[['ZCTA5', 'centroid']]
print('''I did inner merge, on ZCTA5 in zips_texas_borderstates_centroidsand ZIP_CD in active_2016, zip codes in the two dataframes.''')                                     
```

## 4. (a)
```{python}
# Sample
start_time = time.time()
sample_zip = zips_texas_centroids.head(10)
sample_zip = sample_zip.set_geometry('centroid')
zips_withhospital_centroids = zips_withhospital_centroids.set_geometry('centroid')
join_to_hospital_sample = gpd.sjoin_nearest(
    sample_zip,  
    zips_withhospital_centroids,
    how='inner',
    distance_col="distance"
)
end_time = time.time()
print(f'It took {end_time - start_time} seconds.')
print(f'Full merge is expected to take {(end_time - start_time) * (1935/10)} seconds.')
```

## 4. (b)
```{python}
# Full data
start_time = time.time()
zips_texas_centroids = zips_texas_centroids.set_geometry('centroid')
join_to_hospital_full = gpd.sjoin_nearest(
    zips_texas_centroids,  
    zips_withhospital_centroids,
    how='inner',
    distance_col="distance"
)
end_time = time.time()
print(f'It took {end_time - start_time} seconds.')
print('The actual time taken is far more shorter than estimated.')
```

## 4. (c)
From the .prj file, we can see that the unit is specified in degrees. The UNIT parameter is defined as "Degree",0.017453292519943295, so the unit of this coordinate system is a degree, of latitude and longitude. One degree equals approximately 69 miles.
```{python}
# Convert degrees into miles
join_to_hospital_full['distance_miles'] = join_to_hospital_full['distance'] * 69
```

## 5. 
```{python}
avg_distance = join_to_hospital_full['distance'].mean()
print(avg_distance)
```

## 5. (a) 
It is in degrees. <br>

## 5. (b) 
```{python}
avg_distance = join_to_hospital_full['distance'].mean()
avg_distance_mile = avg_distance * 69
print(f'The average distance in miles is {avg_distance_mile:.2f} miles.')
```

This result makes sense because according to info page provided, the number of rural hospital closures has increased significantly in recent years. Since Texas is a state where a substantial share of population living in relatively remote rural areas, it is possible that the average distance to hospital is relatively long. <br>

## 5. (c) 
```{python}
# Merge polygons back to join data
join_to_hospital_full['ZCTA5_left'] = join_to_hospital_full['ZCTA5_left'].astype(str)
gdf['ZCTA5'] = gdf['ZCTA5'].astype(str)
merged_gdf = pd.merge(join_to_hospital_full, gdf[['ZCTA5', 'geometry']],      left_on='ZCTA5_left', right_on='ZCTA5')
merged_gdf['distance'] = merged_gdf['distance'] * 69
merged_gdf = merged_gdf.set_geometry('geometry')

# Form the plot
fig, ax = plt.subplots(1, 1, figsize=(12, 12))
plot = merged_gdf.plot(column='distance', cmap='YlGnBu', legend=True, 
                      legend_kwds={'label': 'Distance to Nearest Hospital (miles)', 'orientation': "vertical", 
                      'format': '%.2f'}, ax=ax)
ax.set_title('Texas ZIP Codes by Distance to Nearest Hospital', fontsize=15, pad=15)
ax.set_axis_off()

plt.show()
```


# Effects of closures on access in Texas (15 pts)

## 1. 
Here we will use the further cleaned dataset of corrected closure which takes ZIP codes with no hospitals reappearing into consideration. (Another version is by simple non-decrease approach, resulting in a corrected closure of 77 hospitals.)
```{python}
zip_vs_counts = corrected_closed.groupby(
    "ZIP_CD").size().reset_index(name="Closure_Count")
# Restrict to Texas
zip_vs_counts["ZIP_CD"] = zip_vs_counts["ZIP_CD"].astype(int).astype(str)
zip_vs_counts = zip_vs_counts[
    zip_vs_counts["ZIP_CD"].str.startswith(("75", "76", "77", "78", "79"))
].reset_index(drop=True)
zip_vs_counts
```
## 2. 
For the choropleth:
```{python}
# Create a GeoDataFrame
gdf["ZCTA5"] = gdf["ZCTA5"].astype(str)
gdf_zip_counts = gdf.merge(zip_vs_counts, left_on="ZCTA5", right_on="ZIP_CD", how="left").fillna(0)
gdf_zip_counts["Closure_Count"] = gdf_zip_counts["Closure_Count"].astype(float)

# Make the Plot
cmap = ListedColormap(["lightgrey", "yellow", "red"])

fig, ax = plt.subplots(figsize=(10, 10))

ax = gdf_zip_counts.plot(
    column="Closure_Count",
    cmap=cmap,
    legend=True,
    categorical=True,
    legend_kwds={"loc": "lower left"},
    linewidth=0.1,
    edgecolor="black",
    ax=ax
)
ax.set_title("Texas ZIP Codes with Hospital Closures")

legend_labels = {"0.0": "Not Affected", "1.0": "Affected by 1 Closure", "2.0": "Affected by 2 Closures"}
for txt in ax.get_legend().get_texts():
    txt.set_text(legend_labels.get(txt.get_text(), txt.get_text()))

plt.axis("off")
plt.show()
```
For the number of directly affected zip codes:
```{python}
number_affected_zip = len(zip_vs_counts)
print(f"There are {number_affected_zip} ZIP codes directly affected by a closure in 2016-2019.")
```
## 3. 
```{python}
# Create a GeoDataFrame of directly affected ZIP codes
gdf_directly_affected = gdf_zip_counts[gdf_zip_counts["Closure_Count"] > 0]

# Apply 10-mile buffer
buffer_degree = 10/69  # convert 10 miles into degrees
buffered_zones = gdf_directly_affected.copy()
buffered_zones["geometry"] = buffered_zones.geometry.buffer(buffer_degree)

# Spatial join to indentify all ZIP codes within the 10-mile buffer
gdf_indirectly_affected = gpd.sjoin(
    gdf, buffered_zones, how="inner", predicate="intersects")
gdf_indirectly_affected = gdf_indirectly_affected[~gdf_indirectly_affected["ZCTA5_left"].isin(
    gdf_directly_affected["ZCTA5"])]

# Find how many
gdf_indirectly_affected["ZCTA5_left"].nunique()
```
Therefore, there are 491 ZIP codes that are indirectly affected.

4. 
```{python}
# Divide ZIP codes into 3 status
gdf["Status"] = "Not Affected"

gdf.loc[gdf["ZCTA5"].isin(gdf_directly_affected["ZCTA5"]), "Status"] = "Directly Affected"

gdf.loc[gdf["ZCTA5"].isin(gdf_indirectly_affected["ZCTA5_left"]), "Status"] = "Indirectly Affected"

# Step 3: Plot the choropleth
cmap = ListedColormap(["red", "orange", "lightgrey"])

fig, ax = plt.subplots(figsize=(10, 10))
gdf.plot(
    column="Status",
    cmap=cmap,
    legend=True,
    legend_kwds={
        "title": "ZIP Code Status",
        "loc": "upper right",
        "labels": [ "Directly Affected", "Indirectly Affected","Not Affected"]
    },
    edgecolor="black",
    linewidth=0.1,
    ax=ax
)
ax.set_title("Texas ZIP Codes Affected by Hospital Closures")

plt.axis("off")
plt.show()
```

# Reflecting on the exercise (10 pts) 
## 1.
The "first-pass" method can lead to several issues:  
-  Overly Simplistic Comparison: In a true closure scenario, the number of active hospitals in the closure year already decreases. Therefore, even if the active hospital count in the year after is larger or equal to the closure year, it is natural and doesn’t necessarily indicate a merger/acquisition.  
-  Inability to Confirm 2019 Closures: This method doesn’t allow us to confirm for 2019 closures because we lack data for the following year. As a result, this will misidentify mergers and overestimate closures.  
-  False Flags Due to New Openings: In cases where a hospital truly closes but a new facility opens in the same zip code the following year, this method would incorrectly suggest that the closure was a merger/acquisition. This results in a misclassification of genuine closures, especially in areas with frequent new hospital openings.  
To do a better job at confirming closures, firstly, for each facility that is no longer active, a closer inspection of the termination code would help clarify whether the hospital ceased operations due to closure or some other reason. Also, we can utilize additional databases or state health department records to validate closure status, especially for hospitals with ambiguous termination codes. Besides, using longitudinal data to track hospitals over multiple years would also be helpful.


## 2.
The definitions of directly and indirectly affected areas make sense, as local closures have an immediate impact that diminishes over distance, thus indirectly affecting nearby areas. <br>

However, this measure fails to consider the actual distance to hospitals and the number of hospitals within each zip code. For instance, the impact of one hospital closing in a zip code with eight hospitals is far less significant than in a zip code with only one hospital. Moreover, the 10-mile buffer radius does not accurately reflect the varied distances across Texas zip codes to the nearest hospitals; some zip codes inherently have hospitals, whereas others are more than 100 miles away from the nearest facility. For zip codes that already have hospitals, the closure of a nearby hospital has a much smaller impact than those zip codes where the nearest hospital is extremely far away. Therefore, this measure fails to reflect the real changes in zip-code-level hospital access accurately. <br>

To improve this measure, I suggest layering the map of zip codes with hospital closures with maps showing distances to the nearest hospital and the number of active hospitals. Specifically, I recommend displaying three maps side by side: the first map only marking zip codes with hospital closures, the second map layering these zip codes as points on a map showing distances to the nearest hospital, and the third map layering these zip codes as points on a map showing the number of active hospitals. This way, readers can understand not only which zip codes have experienced hospital closures but also gauge the actual impact of these closures.